{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75559b6ba7354901a23d07608fbba688",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "n = 4\n",
    "rc = ipp.Cluster(engines=\"mpi\", n=n).start_and_connect_sync()\n",
    "view = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:2] MPI rank: 2/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] MPI rank: 0/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] MPI rank: 3/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] MPI rank: 1/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpi4py import MPI\n",
    "import openpyxl\n",
    "import copy\n",
    "np.random.seed(482)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "mpi = MPI.COMM_WORLD\n",
    "bcast = mpi.bcast\n",
    "barrier = mpi.barrier\n",
    "rank = mpi.rank\n",
    "size = mpi.size\n",
    "print(\"MPI rank: %i/%i\" % (mpi.rank, mpi.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# Implement Custom Loss Function\n",
    "@tf.function\n",
    "def consensus_loss(y_true, y_pred, z, l2):\n",
    "    \n",
    "    # local error\n",
    "    local_loss = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "    # consensus error\n",
    "    consensus_loss = tf.keras.losses.CategoricalCrossentropy()(z, y_pred)\n",
    "        \n",
    "    return local_loss + consensus_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " %%px --block\n",
    "# Play around with this more\n",
    "def set_learning_rate1(optimizer, epoch):\n",
    "    if epoch <= 30:\n",
    "        optimizer.lr = 0.0025\n",
    "    if 30 < epoch <= 100:\n",
    "        optimizer.lr = 0.0015\n",
    "    elif 100 < epoch <= 200:\n",
    "        optimizer.lr = 0.001\n",
    "    elif 200 < epoch <= 300:\n",
    "        optimizer.lr = 0.0005\n",
    "    elif 300 < epoch <= 400:\n",
    "        optimizer.lr = 0.00045\n",
    "    elif 400 < epoch <= 450:\n",
    "        optimizer.lr = 0.00005\n",
    "    else:\n",
    "        optimizer.lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " %%px --block\n",
    "# Play around with this more\n",
    "def set_learning_rate2(optimizer, epoch):\n",
    "    if epoch >= 1:\n",
    "        optimizer.lr = optimizer.lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def average_models(model, local_update, layer_shapes, layer_sizes):\n",
    "    model_weights = model.get_weights()\n",
    "    # flatten tensor weights\n",
    "    coordinate_weights = flatten_weights(model_weights)\n",
    "    local_weights = flatten_weights(local_update)\n",
    "    next_weights = unflatten_weights(np.average([coordinate_weights, local_weights], axis=0),\n",
    "                                           layer_shapes, layer_sizes)\n",
    "    # update model weights to average\n",
    "    model.set_weights(next_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def get_model_architecture(model):\n",
    "    # find shape and total elements for each layer of the resnet model\n",
    "    model_weights = model.get_weights()\n",
    "    layer_shapes = []\n",
    "    layer_sizes = []\n",
    "    for i in range(len(model_weights)):\n",
    "        layer_shapes.append(model_weights[i].shape)\n",
    "        layer_sizes.append(model_weights[i].size)\n",
    "    return layer_shapes, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def flatten_weights(weight_list):\n",
    "    return np.concatenate(list(weight_list[i].flatten() for i in range(len(weight_list))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def unflatten_weights(flat_weights, layer_shapes, layer_sizes):\n",
    "    unflatten_model = []\n",
    "    start_idx = 0\n",
    "    end_idx = 0\n",
    "    for i in range(len(layer_shapes)):\n",
    "        layer_size = layer_sizes[i]\n",
    "        end_idx += layer_size\n",
    "        unflatten_model.append(flat_weights[start_idx:end_idx].reshape(layer_shapes[i]))\n",
    "        start_idx += layer_size\n",
    "    return unflatten_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "\n",
    "def data_pre_process(rank, size, train_pct, train_bs, test_bs, coordination_size, coord_bs):\n",
    "\n",
    "    # read in CSV data\n",
    "    # 13 min load time...\n",
    "    raw_df_data = pd.read_excel(\"Data/CMU_SynTraffic_2022.xlsx\")\n",
    "\n",
    "    # add one-hot encoding of the sub-labels\n",
    "    onehot = pd.get_dummies(raw_df_data['Application Type'])\n",
    "    raw_df = pd.concat([raw_df_data, onehot], axis=1, join='inner')\n",
    "\n",
    "    # label dataframe\n",
    "    traffic_categories = raw_df['Traffic Type'].unique()\n",
    "    tc = dict(zip(traffic_categories, range(len(traffic_categories))))\n",
    "    class_attack = raw_df.Label.map(lambda a: tc[a])\n",
    "    raw_df['Traffic Type'] = class_attack\n",
    "\n",
    "    # shuffle dataset\n",
    "    raw_df = raw_df.sample(frac=1)\n",
    "\n",
    "    # extract features\n",
    "    non_normalized_df = raw_df.drop(['Application Type', 'Traffic Type', 'Data Source'], axis=1)\n",
    "\n",
    "    # extract labels\n",
    "    labels = raw_df['Traffic Type']\n",
    "\n",
    "    # normalize the feature dataframe\n",
    "    normalized_df = non_normalized_df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "    # create coordination set\n",
    "    coord_x = tf.convert_to_tensor(normalized_df.iloc[:coordination_size,:])\n",
    "    coord_y = tf.convert_to_tensor(labels[:coordination_size])\n",
    "    coordination_set = tf.data.Dataset.from_tensor_slices((coord_x,coord_y)).batch(coord_bs)\n",
    "\n",
    "    # get data info\n",
    "    num_inputs = len(normalized_df.columns.to_list())\n",
    "    num_outputs = len(traffic_categories)\n",
    "    \n",
    "    # Split training data amongst workers\n",
    "    worker_data = np.array_split(normalized_df.iloc[coordination_size:,:], size)[rank]\n",
    "    worker_label = np.array_split(labels[coordination_size:], size)[rank]\n",
    "    \n",
    "    # create train/test split\n",
    "    num_data = len(worker_label)\n",
    "    num_train =  int(num_data * train_pct)\n",
    "    # train\n",
    "    train_x = tf.convert_to_tensor(worker_data.iloc[:num_train,:])\n",
    "    train_y = tf.convert_to_tensor(worker_label[:num_train])\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(train_bs)\n",
    "    # test\n",
    "    test_x = tf.convert_to_tensor(worker_data.iloc[num_train:,:])\n",
    "    test_y = tf.convert_to_tensor(worker_label[num_train:])\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((test_x,test_y)).batch(test_bs)\n",
    "    \n",
    "    # full training set\n",
    "    full_train_data = tf.convert_to_tensor(normalized_df)\n",
    "    full_train_label = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    return train_set, test_set, coordination_set, full_train_data, full_train_label, num_inputs, num_outputs"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def train(model, rank, size, lossF, optimizer, train_dataset, coordination_dataset, epochs, \n",
    "          coord_batch_size, batches, num_outputs, layer_shapes, layer_sizes, l2):\n",
    "\n",
    "    acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        set_learning_rate2(optimizer, epoch)\n",
    "\n",
    "        # Forward Pass of Coordination Set (get z)\n",
    "        send_predicted = np.zeros((num_outputs*coord_batch_size, batches), dtype=np.float32)\n",
    "        recv_avg_pred = np.zeros((num_outputs*coord_batch_size, batches), dtype=np.float32)\n",
    "        for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "            pred = model(c_data, training=True)\n",
    "            send_predicted[:, c_batch_idx] = pred.numpy().flatten()\n",
    "\n",
    "        # Communication Process Here\n",
    "        MPI.COMM_WORLD.Allreduce(send_predicted, recv_avg_pred, op=MPI.SUM)\n",
    "        recv_avg_pred = recv_avg_pred/size\n",
    "\n",
    "        # save initial model\n",
    "        start_model = copy.deepcopy(model.get_weights())\n",
    "\n",
    "        # Local Training\n",
    "        for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_p = model(data, training=True)\n",
    "                loss_val = lossF(y_true=target, y_pred=y_p)\n",
    "            grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            acc_metric.update_state(target, y_p)\n",
    "            loss_metric.update_state(target, y_p)\n",
    "\n",
    "        # save model after local update\n",
    "        local_model = copy.deepcopy(model.get_weights())\n",
    "\n",
    "        # reset model weights\n",
    "        model.set_weights(start_model)\n",
    "        \n",
    "        # Consensus Training\n",
    "        for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                c_yp = model(c_data, training=True)\n",
    "                loss_val = consensus_loss(y_true=c_target, y_pred=c_yp,\n",
    "                                           z=recv_avg_pred[:, c_batch_idx].reshape(coord_batch_size, num_outputs),\n",
    "                                           l2=l2)\n",
    "            grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        # update model weights\n",
    "        average_models(model, local_model, layer_shapes, layer_sizes)\n",
    "        \n",
    "        print('(Rank %d) Epoch %d: Training Accuracy is %0.4f, Loss is %0.4f' % (rank, epoch, acc_metric.result(), \\\n",
    "                                                                                loss_metric.result()))\n",
    "        loss_metric.reset_states()\n",
    "        acc_metric.reset_states()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Run Experiments"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "train_pct = 0.8\n",
    "train_bs = 64\n",
    "test_bs = 64\n",
    "coord_bs = 32\n",
    "coordination_size = coord_bs**2\n",
    "coord_num_batches = int(np.ceil(coordination_size/coord_bs))\n",
    "\n",
    "train_set, test_set, coord_set, full_train_x, full_train_y, num_inputs, num_outputs = data_pre_process(rank, size, train_pct, \n",
    "                                                                                                       train_bs, test_bs,\n",
    "                                                                                                       coordination_size, \n",
    "                                                                                                       coord_bs)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# multi classification model\n",
    "if rank == 0 or rank == 1:\n",
    "    multi_model = tf.keras.Sequential()\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_inputs,)))\n",
    "    multi_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(num_outputs, activation='softmax'))\n",
    "else:\n",
    "    multi_model = tf.keras.Sequential()\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(num_inputs,)))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(num_outputs, activation='softmax'))\n",
    "\n",
    "# Initialize Local Loss Function\n",
    "lossF = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# model architecture\n",
    "layer_shapes, layer_sizes = get_model_architecture(multi_model)\n",
    "\n",
    "# l2 penalty\n",
    "l2 = 0.1\n",
    "\n",
    "# epochs\n",
    "epochs = 10\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "train(multi_model, rank, size, lossF, optimizer, train_set, \n",
    "      coord_set, epochs, coord_bs, coord_num_batches, num_outputs, layer_shapes, layer_sizes, l2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "predictions = multi_model.predict(full_train_x)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "pred = tf.math.argmax(predictions, axis=1)\n",
    "train_confusion_mtx = tf.math.confusion_matrix(full_train_y, pred)\n",
    "# normalize confusion matrix\n",
    "train_confusion_mtx = train_confusion_mtx / tf.reduce_sum(train_confusion_mtx, 0).numpy()\n",
    "train_confusion_mtx = tf.where(tf.math.is_nan(train_confusion_mtx), tf.zeros_like(train_confusion_mtx), train_confusion_mtx)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "%%px --block\n",
    "attack_labels = ['Tor', 'VPN', 'Regular']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(train_confusion_mtx,\n",
    "            xticklabels=attack_labels,\n",
    "            yticklabels=attack_labels,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Worker %d on CIC-Darknet2020 Data' % (rank+1))\n",
    "plt.show()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "%%px --block\n",
    "\n",
    "%%px --block\n",
    "# read in CSV data\n",
    "# 13 min load time...\n",
    "raw_df_data = pd.read_excel(\"Data/CMU_SynTraffic_2022.xlsx\")\n",
    "\n",
    "train_pct = 0.8\n",
    "train_bs = 64\n",
    "test_bs = 64\n",
    "coord_bs = 32\n",
    "coordination_size = coord_bs**2\n",
    "coord_num_batches = int(np.ceil(coordination_size/coord_bs))\n",
    "\n",
    "# add one-hot encoding of the sub-labels\n",
    "onehot = pd.get_dummies(raw_df_data['Application Type'])\n",
    "raw_df = pd.concat([raw_df_data, onehot], axis=1, join='inner')\n",
    "\n",
    "# label dataframe\n",
    "traffic_categories = raw_df['Traffic Type'].unique()\n",
    "tc = dict(zip(traffic_categories, range(len(traffic_categories))))\n",
    "class_attack = raw_df['Traffic Type'].map(lambda a: tc[a])\n",
    "raw_df['Traffic Type'] = class_attack\n",
    "\n",
    "# shuffle dataset\n",
    "raw_df = raw_df.sample(frac=1)\n",
    "\n",
    "# extract features\n",
    "non_normalized_df = raw_df.drop(['Application Type', 'Traffic Type', 'Data Source'], axis=1)\n",
    "\n",
    "# extract labels\n",
    "labels = raw_df['Traffic Type']\n",
    "\n",
    "# normalize the feature dataframe\n",
    "normalized_df = non_normalized_df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "# create coordination set\n",
    "coord_x = tf.convert_to_tensor(normalized_df.iloc[:coordination_size,:])\n",
    "coord_y = tf.convert_to_tensor(labels[:coordination_size])\n",
    "coordination_set = tf.data.Dataset.from_tensor_slices((coord_x,coord_y)).batch(coord_bs)\n",
    "\n",
    "# get data info\n",
    "num_inputs = len(normalized_df.columns.to_list())\n",
    "num_outputs = len(traffic_categories)\n",
    "\n",
    "# Split training data amongst workers\n",
    "worker_data = np.array_split(normalized_df.iloc[coordination_size:,:], size)[rank]\n",
    "worker_label = np.array_split(labels[coordination_size:], size)[rank]\n",
    "\n",
    "# create train/test split\n",
    "num_data = len(worker_label)\n",
    "num_train =  int(num_data * train_pct)\n",
    "# train\n",
    "train_x = tf.convert_to_tensor(worker_data.iloc[:num_train,:])\n",
    "train_y = tf.convert_to_tensor(worker_label[:num_train])\n",
    "train_set = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(train_bs)\n",
    "# test\n",
    "test_x = tf.convert_to_tensor(worker_data.iloc[num_train:,:])\n",
    "test_y = tf.convert_to_tensor(worker_label[num_train:])\n",
    "test_set = tf.data.Dataset.from_tensor_slices((test_x,test_y)).batch(test_bs)\n",
    "\n",
    "# full training set\n",
    "full_train_data = tf.convert_to_tensor(normalized_df)\n",
    "full_train_label = tf.convert_to_tensor(labels)\n",
    "'''"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}