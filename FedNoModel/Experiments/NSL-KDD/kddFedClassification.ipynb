{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "## Used Code From https://www.kaggle.com/code/timgoodfellow/nsl-kdd-explorations to get NSL-KDD setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b28713e698485cab764735060a96bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "n = 4\n",
    "rc = ipp.Cluster(engines=\"mpi\", n=n).start_and_connect_sync()\n",
    "view = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] MPI rank: 0/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] MPI rank: 2/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] MPI rank: 3/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] MPI rank: 1/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from mpi4py import MPI\n",
    "np.random.seed(482)\n",
    "\n",
    "mpi = MPI.COMM_WORLD\n",
    "bcast = mpi.bcast\n",
    "barrier = mpi.barrier\n",
    "rank = mpi.rank\n",
    "size = mpi.size\n",
    "print(\"MPI rank: %i/%i\" % (mpi.rank, mpi.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# Implement Custom Loss Function\n",
    "@tf.function\n",
    "def consensus_loss(y_true, y_pred, z, l2):\n",
    "\n",
    "    # local error\n",
    "    local_loss = tf.keras.losses.BinaryCrossentropy()(y_true, y_pred)\n",
    "    # local_error = y_true - y_pred\n",
    "    # local_square_error = tf.square(local_error)\n",
    "    # local_mse = tf.reduce_mean(local_square_error)\n",
    "\n",
    "    # consensus loss error\n",
    "    consensus_loss = l2*tf.keras.losses.BinaryCrossentropy()(z, y_pred)\n",
    "    # consensus_error = z - y_pred\n",
    "    # consensus_square_error = tf.square(consensus_error)\n",
    "    # consensus_mse = l2*tf.reduce_sum(consensus_square_error)\n",
    "\n",
    "    return local_loss + consensus_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " %%px --block\n",
    "# Play around with this more\n",
    "def set_learning_rate1(optimizer, epoch):\n",
    "    if epoch <= 30:\n",
    "        optimizer.lr = 0.0025\n",
    "    if 30 < epoch <= 100:\n",
    "        optimizer.lr = 0.0015\n",
    "    elif 100 < epoch <= 200:\n",
    "        optimizer.lr = 0.001\n",
    "    elif 200 < epoch <= 300:\n",
    "        optimizer.lr = 0.0005\n",
    "    elif 300 < epoch <= 400:\n",
    "        optimizer.lr = 0.00045\n",
    "    elif 400 < epoch <= 450:\n",
    "        optimizer.lr = 0.00005\n",
    "    else:\n",
    "        optimizer.lr = 0.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " %%px --block\n",
    "# Play around with this more\n",
    "def set_learning_rate2(optimizer, epoch):\n",
    "    if epoch >= 1:\n",
    "        optimizer.lr = optimizer.lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def data_pre_process(rank, size, coordination_size, coord_bs, train_bs):\n",
    "\n",
    "    def map_attack(attack):\n",
    "\n",
    "        # attack classifications\n",
    "        dos_attacks = ['apache2','back','land','neptune','mailbomb','pod','processtable',\n",
    "                       'smurf','teardrop','udpstorm','worm']\n",
    "        probe_attacks = ['ipsweep','mscan','nmap','portsweep','saint','satan']\n",
    "        privilege_attacks = ['buffer_overflow','loadmdoule','perl','ps','rootkit','sqlattack','xterm']\n",
    "        access_attacks = ['ftp_write','guess_passwd','http_tunnel','imap','multihop',\n",
    "                          'named','phf','sendmail','snmpgetattack','snmpguess','spy',\n",
    "                          'warezclient','warezmaster','xclock','xsnoop']\n",
    "\n",
    "        if attack in dos_attacks:\n",
    "            # dos_attacks map to 1\n",
    "            attack_type = 1\n",
    "        elif attack in probe_attacks:\n",
    "            # probe_attacks mapt to 2\n",
    "            attack_type = 2\n",
    "        elif attack in privilege_attacks:\n",
    "            # privilege escalation attacks map to 3\n",
    "            attack_type = 3\n",
    "        elif attack in access_attacks:\n",
    "            # remote access attacks map to 4\n",
    "            attack_type = 4\n",
    "        else:\n",
    "            # normal maps to 0\n",
    "            attack_type = 0\n",
    "        return attack_type\n",
    "\n",
    "    # load data with column names\n",
    "    colnames = ['duration', 'protocol_type', 'service', 'flag', 'src_bytes', 'dst_bytes', 'land',\n",
    "            'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "            'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "            'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "            'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "            'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "            'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "            'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "            'dst_host_srv_rerror_rate', 'attack_type', 'level']\n",
    "    train_df = pd.read_csv(\"Data/KDDTrain+.txt\", names=colnames, sep=\",\")\n",
    "    test_df = pd.read_csv(\"Data/KDDTest+.txt\", names=colnames, sep=\",\")\n",
    "\n",
    "    # make all floats 32 (for tensorflow memory purposes)\n",
    "    train_df[train_df.select_dtypes(np.float64).columns] = train_df.select_dtypes(np.float64).astype(np.float32)\n",
    "    test_df[test_df.select_dtypes(np.float64).columns] = test_df.select_dtypes(np.float64).astype(np.float32)\n",
    "\n",
    "    # Create an attack flag: map normal behavior to 0, all attacks to 1\n",
    "    is_attack_train = train_df.attack_type.map(lambda a: 0 if a == 'normal' else 1)\n",
    "    is_attack_test = test_df.attack_type.map(lambda a: 0 if a == 'normal' else 1)\n",
    "    train_df['attack_flag'] = is_attack_train\n",
    "    test_df['attack_flag'] = is_attack_test\n",
    "\n",
    "    attack_labels = ['Normal','DoS','Probe','Privilege','Access']\n",
    "\n",
    "    # map the data and join to the data set\n",
    "    attack_map = train_df.attack_type.apply(map_attack)\n",
    "    train_df['attack_map'] = attack_map\n",
    "\n",
    "    test_attack_map = test_df.attack_type.apply(map_attack)\n",
    "    test_df['attack_map'] = test_attack_map\n",
    "\n",
    "    onehot_features = ['protocol_type', 'service', 'flag']\n",
    "    other_features = ['duration', 'src_bytes', 'dst_bytes', 'land',\n",
    "                'wrong_fragment', 'urgent', 'hot', 'num_failed_logins', 'logged_in', 'num_compromised',\n",
    "                'root_shell', 'su_attempted', 'num_root', 'num_file_creations', 'num_shells', 'num_access_files',\n",
    "                'num_outbound_cmds', 'is_host_login', 'is_guest_login', 'count', 'srv_count', 'serror_rate',\n",
    "                'srv_serror_rate', 'rerror_rate', 'srv_rerror_rate', 'same_srv_rate', 'diff_srv_rate',\n",
    "                'srv_diff_host_rate', 'dst_host_count', 'dst_host_srv_count', 'dst_host_same_srv_rate',\n",
    "                'dst_host_diff_srv_rate', 'dst_host_same_src_port_rate', 'dst_host_srv_diff_host_rate',\n",
    "                'dst_host_serror_rate', 'dst_host_srv_serror_rate', 'dst_host_rerror_rate',\n",
    "                'dst_host_srv_rerror_rate', 'level']\n",
    "    train_onehot = pd.get_dummies(train_df[onehot_features])\n",
    "    test_onehot = pd.get_dummies(test_df[onehot_features])\n",
    "\n",
    "    test_index = np.arange(len(test_df.index))\n",
    "    column_diffs = list(set(train_onehot.columns.values)-set(test_onehot.columns.values))\n",
    "    diff_df = pd.DataFrame(0, index=test_index, columns=column_diffs)\n",
    "    column_order = train_onehot.columns.to_list()\n",
    "    test_temp = test_onehot.join(diff_df)\n",
    "    test_final = test_temp[column_order].fillna(0)\n",
    "\n",
    "    test_set = test_final.join(test_df[other_features])\n",
    "    train_set = train_onehot.join(train_df[other_features])\n",
    "\n",
    "    train_set_norm = train_set.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    test_set_norm = test_set.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "    train_set_norm = train_set_norm.fillna(0)\n",
    "    test_set_norm = test_set_norm.fillna(0)\n",
    "    num_inputs = len(train_set.columns.to_list())\n",
    "    num_outputs_multi = len(attack_labels)\n",
    "\n",
    "    # split train into a coordination set\n",
    "    coord_set = train_set_norm.iloc[:coordination_size,:]\n",
    "    whole_train_set = train_set_norm.iloc[coordination_size:,:]\n",
    "\n",
    "    # test labels\n",
    "    test_binary_y = tf.convert_to_tensor(test_df['attack_flag'])\n",
    "    test_multi_y = tf.convert_to_tensor(test_df['attack_map'].to_list())\n",
    "\n",
    "    # Split training data amongst workers\n",
    "    worker_train_set_norm = np.array_split(whole_train_set, size)[rank]\n",
    "\n",
    "    # create our train labels\n",
    "    train_attack_map_list_binary = train_df['attack_flag'].to_list()\n",
    "    train_attack_map_list_multi = train_df['attack_map'].to_list()\n",
    "\n",
    "    # split coordination set labels\n",
    "    coordination_binary_y = tf.convert_to_tensor(train_attack_map_list_binary[:coordination_size])\n",
    "    coordination_multi_y = tf.convert_to_tensor(train_attack_map_list_multi[:coordination_size])\n",
    "    whole_train_label_b = train_attack_map_list_binary[coordination_size:]\n",
    "    whole_train_label_m = train_attack_map_list_multi[coordination_size:]\n",
    "\n",
    "    worker_train_label_binary = np.array_split(whole_train_label_b, size)[rank]\n",
    "    worker_train_label_multi = np.array_split(whole_train_label_m, size)[rank]\n",
    "    train_binary_y = tf.convert_to_tensor(worker_train_label_binary)\n",
    "    train_multi_y = tf.convert_to_tensor(worker_train_label_multi)\n",
    "\n",
    "    # make data into tensors\n",
    "    train_set = tf.convert_to_tensor(worker_train_set_norm)\n",
    "    test_set = tf.convert_to_tensor(test_set_norm)\n",
    "    coordination_set = tf.convert_to_tensor(coord_set)\n",
    "\n",
    "    # create tensorflow dataset for test and train\n",
    "    training_binary = tf.data.Dataset.from_tensor_slices((train_set, train_binary_y)).batch(train_bs)\n",
    "    training_multi = tf.data.Dataset.from_tensor_slices((train_set, train_multi_y)).batch(train_bs)\n",
    "    test_binary = tf.data.Dataset.from_tensor_slices((test_set, test_binary_y)).batch(train_bs)\n",
    "    test_multi = tf.data.Dataset.from_tensor_slices((test_set, test_multi_y)).batch(train_bs)\n",
    "    coordination_binary = tf.data.Dataset.from_tensor_slices((coordination_set, coordination_binary_y)).batch(coord_bs)\n",
    "    coordination_multi = tf.data.Dataset.from_tensor_slices((coordination_set, coordination_multi_y)).batch(coord_bs)\n",
    "\n",
    "    return training_binary, test_binary, coordination_binary, training_multi, test_multi, coordination_multi, num_inputs,  num_outputs_multi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def train(model, rank, size, lossF, optimizer, train_dataset, coordination_dataset, epochs, coord_batch_size, batches, isBinary):\n",
    "\n",
    "    if isBinary:\n",
    "        loss_metric = tf.keras.metrics.BinaryAccuracy()\n",
    "    else:\n",
    "        loss_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "\n",
    "        # Adjust learning rate\n",
    "        set_learning_rate2(optimizer, epoch)\n",
    "\n",
    "        # Local Training\n",
    "        for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_p = model(data, training=True)\n",
    "                loss_val = lossF(y_true=target, y_pred=y_p)\n",
    "            grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            loss_metric.update_state(target, y_p)\n",
    "\n",
    "        # Forward Pass of Coordination Set\n",
    "        send_predicted = np.zeros((coord_batch_size, batches), dtype=np.float32)\n",
    "        recv_avg_pred = np.zeros((coord_batch_size, batches), dtype=np.float32)\n",
    "        for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "            pred = model(c_data, training=True)\n",
    "            send_predicted[:, c_batch_idx] = pred.numpy().flatten()\n",
    "\n",
    "        # Communication Process Here\n",
    "        MPI.COMM_WORLD.Allreduce(send_predicted, recv_avg_pred, op=MPI.SUM)\n",
    "        recv_avg_pred = recv_avg_pred/size\n",
    "\n",
    "        # Consensus Training\n",
    "        for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "            with tf.GradientTape() as tape:\n",
    "                c_yp = model(c_data, training=True)\n",
    "                loss_val = consensus_loss(y_true=c_target, y_pred=c_yp,\n",
    "                                           z=recv_avg_pred[:, c_batch_idx].reshape(coord_batch_size, 1),\n",
    "                                           l2=0.1)\n",
    "            grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # loss_metric.update_state(c_target, c_yp)\n",
    "        print('(Rank %d) Training Loss for Epoch %d: %0.4f' % (rank, epoch, loss_metric.result()))\n",
    "        loss_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1dbda1ec856949d191730c1f64b31e37",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "isBinary = True\n",
    "epochs = 10\n",
    "train_bs = 64\n",
    "coord_bs = 32\n",
    "coord_examples = 32*coord_bs\n",
    "coord_num_batches = int(np.ceil(coord_examples/coord_bs))\n",
    "train_data_b, test_data_b, coord_data_b, train_data_m, test_data_m, coord_data_m, num_inputs, num_outputs_multi =  data_pre_process(rank, size, coord_examples, coord_bs, train_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# binary classification model\n",
    "if rank == 0 or rank == 1:\n",
    "    binary_model = tf.keras.Sequential()\n",
    "    binary_model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_inputs,)))\n",
    "    binary_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "else: \n",
    "    binary_model = tf.keras.Sequential()\n",
    "    binary_model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(num_inputs,)))\n",
    "    binary_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    binary_model.add(tf.keras.layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "# Initialize Local Loss Function\n",
    "lossF = tf.keras.losses.BinaryCrossentropy()\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a124e3594e4d4b138e024a737ff868d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] (Rank 0) Training Loss for Epoch 0: 0.9841\n",
       "(Rank 0) Training Loss for Epoch 1: 0.9931\n",
       "(Rank 0) Training Loss for Epoch 2: 0.9927\n",
       "(Rank 0) Training Loss for Epoch 3: 0.9952\n",
       "(Rank 0) Training Loss for Epoch 4: 0.9961\n",
       "(Rank 0) Training Loss for Epoch 5: 0.9960\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "train(binary_model, rank, size, lossF, optimizer, train_data_b, coord_data_b, epochs, coord_bs, coord_num_batches, isBinary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "binary_model.compile(loss=lossF, optimizer=optimizer, metrics=[tf.keras.metrics.BinaryAccuracy()])\n",
    "binary_model.evaluate(test_data_b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# multi classification model\n",
    "multi_model = tf.keras.Sequential()\n",
    "multi_model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_inputs,)))\n",
    "multi_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "multi_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "multi_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "multi_model.add(tf.keras.layers.Dense(num_outputs_multi, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}