{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 390,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "%matplotlib qt\n",
    "np.random.seed(132)\n",
    "tf.keras.backend.set_floatx('float64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 391,
   "outputs": [],
   "source": [
    "# How to implement\n",
    "# 1) Make custom training loop, at beginning of each round the devices test model on coordination set and then\n",
    "#    send the results to all neighboring devices where they are averaged. The\n",
    "\n",
    "# Double training loop. Train first on local data, and then train on coordination set with the consensus loss"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "outputs": [],
   "source": [
    "def synthetic_data2d(n, alpha):\n",
    "    noise_x = alpha*np.random.normal(size=n)\n",
    "    noise_y = alpha*np.random.normal(size=n)\n",
    "    x = np.random.uniform(-2*np.pi, 2*np.pi, size=(n, 2))\n",
    "    y = np.sin(np.cos(x[:, 1]) + noise_x) + np.exp(np.cos(x[:, 0]) + noise_y)\n",
    "    return x, y"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 393,
   "outputs": [],
   "source": [
    "# Hyper-parameters\n",
    "n = 1000\n",
    "alpha = 0.05\n",
    "epochs = 100\n",
    "learning_rate = 0.01\n",
    "# 2d example\n",
    "X, Y = synthetic_data2d(n, alpha)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "outputs": [],
   "source": [
    "# Rescale data between 0 and 1\n",
    "data_max = np.max(X)\n",
    "data_min = np.min(X)\n",
    "X = (X - data_min) / (data_max - data_min)\n",
    "# Split up data\n",
    "train_split = 0.8\n",
    "batch_size = 64\n",
    "num_data = len(Y)\n",
    "train_x = X[0:int(num_data * train_split), :]\n",
    "train_y = Y[0:int(num_data * train_split)]\n",
    "test_x = X[int(num_data * train_split):, :]\n",
    "test_y = Y[int(num_data * train_split):]\n",
    "# convert to tensors\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((train_x, train_y))\n",
    "test_dataset = tf.data.Dataset.from_tensor_slices((test_x, test_y))\n",
    "# shuffle and batch\n",
    "train_dataset = train_dataset.shuffle(int(num_data * train_split)).batch(batch_size)\n",
    "test_dataset = test_dataset.batch(batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 395,
   "outputs": [],
   "source": [
    "# Coordination set construction\n",
    "coord_size = 160\n",
    "c_batch_size = 16\n",
    "true_x = np.random.uniform(-2*np.pi, 2*np.pi, size=(coord_size, 2))\n",
    "true_y = np.sin(np.cos(true_x[:, 1])) + np.exp(np.cos(true_x[:, 0]))\n",
    "coord_max = np.max(true_x)\n",
    "coord_min = np.min(true_x)\n",
    "true_x = (true_x - data_min) / (data_max - data_min)\n",
    "coordination_dataset = tf.data.Dataset.from_tensor_slices((true_x, true_y))\n",
    "coordination_dataset = coordination_dataset.batch(c_batch_size)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 396,
   "outputs": [],
   "source": [
    "# Implement Custom Loss Function\n",
    "@tf.function\n",
    "def consensus_loss(y_true, y_pred, z, l2):\n",
    "    # local error\n",
    "    local_error = y_true - y_pred\n",
    "    local_square_error = tf.square(local_error)\n",
    "    local_mse = tf.reduce_mean(local_square_error)\n",
    "    # consensus error\n",
    "    consensus_error = z - y_pred\n",
    "    consensus_square_error = tf.square(consensus_error)\n",
    "    consensus_mse = l2*tf.reduce_mean(consensus_square_error)\n",
    "    return local_mse + consensus_mse"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 397,
   "outputs": [],
   "source": [
    "# Initialize Model\n",
    "model = tf.keras.Sequential()\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu', input_shape=(2,)))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "model.add(tf.keras.layers.Dense(1))\n",
    "\n",
    "# Initialize Local Loss Function\n",
    "lossF = tf.keras.losses.MeanSquaredError()\n",
    "\n",
    "# Initialize Optimizer\n",
    "# learning_rate = tf.keras.optimizers.schedules.ExponentialDecay(initial_learning_rate=learning_rate, decay_steps=20, decay_rate=.5)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 398,
   "outputs": [],
   "source": [
    "# def scheduler(epoch, lr):\n",
    "#  if epoch < 45:\n",
    "#     return lr\n",
    "#   else:\n",
    "#     return lr * tf.math.exp(-0.05)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [
    "# Training\n",
    "loss_metric = tf.keras.metrics.MeanSquaredError()\n",
    "for epoch in range(epochs):\n",
    "    # Local Training\n",
    "    for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_p = model(data, training=True)\n",
    "            loss_val = lossF(y_true=target, y_pred=y_p)\n",
    "        grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "    # Communication Process Here\n",
    "\n",
    "    # Consensus Training\n",
    "    for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "        with tf.GradientTape() as tape:\n",
    "            c_yp = model(c_data, training=True)\n",
    "            # for now testing, have consensus be just the predicted (so no consensus error)\n",
    "            z = c_yp\n",
    "            loss_val = consensus_loss(y_true=c_target, y_pred=c_yp, z=z, l2=1.0)\n",
    "        grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "        loss_metric.update_state(c_target, c_yp)\n",
    "        # if (c_batch_idx+1) % (coord_size/c_batch_size) == 0:\n",
    "        #    print('Training Loss: %0.4f' % (loss_metric.result()))\n",
    "\n",
    "    loss_metric.reset_states()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 399,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}