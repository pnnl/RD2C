{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting 4 engines with <class 'ipyparallel.cluster.launcher.MPIEngineSetLauncher'>\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e598722078d749a8bf0a8c485a06f501",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/4 [00:00<?, ?engine/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[0, 1, 2, 3]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import ipyparallel as ipp\n",
    "n = 4\n",
    "rc = ipp.Cluster(engines=\"mpi\", n=n).start_and_connect_sync()\n",
    "view = rc[:]\n",
    "rc.ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[stdout:0] MPI rank: 0/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] MPI rank: 2/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] MPI rank: 3/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] MPI rank: 1/4\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from mpi4py import MPI\n",
    "import copy\n",
    "import sys\n",
    "sys.path.append('../../../Decentralized-FL-Framework')\n",
    "from communication import DecentralizedNoModelSGD\n",
    "from comm_weights import flatten_weights, unflatten_weights\n",
    "from network import Graph\n",
    "\n",
    "\n",
    "np.random.seed(482)\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "\n",
    "mpi = MPI.COMM_WORLD\n",
    "bcast = mpi.bcast\n",
    "barrier = mpi.barrier\n",
    "rank = mpi.rank\n",
    "size = mpi.size\n",
    "print(\"MPI rank: %i/%i\" % (mpi.rank, mpi.size))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# Implement Custom Loss Function\n",
    "@tf.function\n",
    "def consensus_loss(y_true, y_pred, z, l2):\n",
    "    \n",
    "    # local error\n",
    "    local_loss = tf.keras.losses.SparseCategoricalCrossentropy()(y_true, y_pred)\n",
    "    # consensus error\n",
    "    consensus_loss = l2*tf.keras.losses.CategoricalCrossentropy()(z, y_pred)\n",
    "        \n",
    "    return local_loss + consensus_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    " %%px --block\n",
    "# Play around with this more\n",
    "def set_learning_rate(optimizer, epoch):\n",
    "    if epoch >= 1:\n",
    "        optimizer.lr = optimizer.lr * tf.math.exp(-0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def average_models(model, local_update, layer_shapes, layer_sizes):\n",
    "    model_weights = model.get_weights()\n",
    "    # flatten tensor weights\n",
    "    coordinate_weights = flatten_weights(model_weights)\n",
    "    local_weights = flatten_weights(local_update)\n",
    "    next_weights = unflatten_weights(np.average([coordinate_weights, local_weights], axis=0),\n",
    "                                           layer_shapes, layer_sizes)\n",
    "    # update model weights to average\n",
    "    model.set_weights(next_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def get_model_architecture(model):\n",
    "    # find shape and total elements for each layer of the resnet model\n",
    "    model_weights = model.get_weights()\n",
    "    layer_shapes = []\n",
    "    layer_sizes = []\n",
    "    for i in range(len(model_weights)):\n",
    "        layer_shapes.append(model_weights[i].shape)\n",
    "        layer_sizes.append(model_weights[i].size)\n",
    "    return layer_shapes, layer_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "\n",
    "def data_pre_process(rank, size, train_pct, train_bs, test_bs, coordination_size, coord_bs):\n",
    "\n",
    "    # read in CSV data\n",
    "    raw_df_data = pd.read_csv(\"Data/Darknet.CSV\", parse_dates=[\"Timestamp\"], on_bad_lines='skip')\n",
    "\n",
    "    # make timestamp numeric (just time of day)\n",
    "    timestamp = raw_df_data[\"Timestamp\"]\n",
    "    raw_df_data[\"Timestamp\"] = timestamp.dt.hour + timestamp.dt.minute/60 + timestamp.dt.second/3600\n",
    "\n",
    "    # remove NaN rows and Inf\n",
    "    raw_df = raw_df_data.replace([np.inf, -np.inf], np.nan)\n",
    "    raw_df.dropna(inplace=True)\n",
    "\n",
    "    # clean up the sub-labels (incorrectly labeled)\n",
    "    raw_df.loc[raw_df['Label.1']=='AUDIO-STREAMING', 'Label.1'] = 'Audio-Streaming'\n",
    "    raw_df.loc[raw_df['Label.1']=='AUDIO-STREAMING', 'Label.1'] = 'Audio-Streaming'\n",
    "    raw_df.loc[raw_df['Label.1']=='Video-streaming', 'Label.1'] = 'Video-Streaming'\n",
    "    raw_df.rename(columns = {\"Label.1\" : \"Subtype\"}, inplace = True)\n",
    "\n",
    "    # add one-hot encoding of the sub-labels\n",
    "    onehot = pd.get_dummies(raw_df['Subtype'])\n",
    "    raw_df = pd.concat([raw_df, onehot], axis=1, join='inner')\n",
    "\n",
    "    # drop IP columns and 0 columns\n",
    "    ip_cols = ['Flow ID', 'Src IP', 'Dst IP', 'Active Mean', 'Active Std', 'Active Max', \n",
    "               'Active Min', 'Subflow Bwd Packets', 'Fwd Bytes/Bulk Avg', 'Fwd Packet/Bulk Avg', \n",
    "               'Fwd Bulk Rate Avg', 'Bwd Bytes/Bulk Avg', 'URG Flag Count', 'CWE Flag Count', \n",
    "               'ECE Flag Count', 'Bwd PSH Flags', 'Fwd URG Flags','Bwd URG Flags']\n",
    "    raw_df.drop(ip_cols, axis=1, inplace=True)\n",
    "\n",
    "    # label dataframe\n",
    "    traffic_categories = raw_df['Label'].unique()\n",
    "    tc = dict(zip(traffic_categories, range(len(traffic_categories))))\n",
    "    class_attack = raw_df.Label.map(lambda a: tc[a])\n",
    "    raw_df['Label'] = class_attack\n",
    "\n",
    "    # shuffle dataset\n",
    "    raw_df = raw_df.sample(frac=1)\n",
    "\n",
    "    # extract features\n",
    "    non_normalized_df = raw_df.drop(['Label', 'Subtype'], axis=1)\n",
    "\n",
    "    # extract labels\n",
    "    labels = raw_df['Label']\n",
    "\n",
    "    # normalize the feature dataframe\n",
    "    normalized_df = non_normalized_df.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
    "\n",
    "    # create coordination set\n",
    "    coord_x = tf.convert_to_tensor(normalized_df.iloc[:coordination_size,:])\n",
    "    coord_y = tf.convert_to_tensor(labels[:coordination_size])\n",
    "    coordination_set = tf.data.Dataset.from_tensor_slices((coord_x,coord_y)).batch(coord_bs)\n",
    "\n",
    "    # get data info\n",
    "    num_inputs = len(normalized_df.columns.to_list())\n",
    "    num_outputs = len(traffic_categories)\n",
    "    \n",
    "    # Split training data amongst workers\n",
    "    worker_data = np.array_split(normalized_df.iloc[coordination_size:,:], size)[rank]\n",
    "    worker_label = np.array_split(labels[coordination_size:], size)[rank]\n",
    "    \n",
    "    # create train/test split\n",
    "    num_data = len(worker_label)\n",
    "    num_train =  int(num_data * train_pct)\n",
    "    # train\n",
    "    train_x = tf.convert_to_tensor(worker_data.iloc[:num_train,:])\n",
    "    train_y = tf.convert_to_tensor(worker_label[:num_train])\n",
    "    train_set = tf.data.Dataset.from_tensor_slices((train_x,train_y)).batch(train_bs)\n",
    "    # test\n",
    "    test_x = tf.convert_to_tensor(worker_data.iloc[num_train:,:])\n",
    "    test_y = tf.convert_to_tensor(worker_label[num_train:])\n",
    "    test_set = tf.data.Dataset.from_tensor_slices((test_x,test_y)).batch(test_bs)\n",
    "    \n",
    "    # full training set\n",
    "    full_train_data = tf.convert_to_tensor(normalized_df)\n",
    "    full_train_label = tf.convert_to_tensor(labels)\n",
    "    \n",
    "    return train_set, test_set, coordination_set, full_train_data, full_train_label, num_inputs, num_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "def train(model, communicator, rank, size, lossF, optimizer, train_dataset, coordination_dataset, epochs, \n",
    "          coord_batch_size, batches, num_outputs, layer_shapes, layer_sizes, l2):\n",
    "\n",
    "    acc_metric = tf.keras.metrics.SparseCategoricalAccuracy()\n",
    "    loss_metric = tf.keras.metrics.SparseCategoricalCrossentropy()\n",
    "        \n",
    "    for epoch in range(epochs):\n",
    "        \n",
    "        # Adjust learning rate\n",
    "        set_learning_rate(optimizer, epoch)\n",
    "\n",
    "        # Forward Pass of Coordination Set (get z)\n",
    "        send_predicted = np.zeros((num_outputs*coord_batch_size, batches), dtype=np.float32)\n",
    "        for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "            pred = model(c_data, training=True)\n",
    "            send_predicted[:, c_batch_idx] = pred.numpy().flatten()\n",
    "\n",
    "        # Communication Process Here\n",
    "        recv_avg_pred, comm_time = communicator.average(send_predicted)\n",
    "\n",
    "        # save initial model\n",
    "        start_model = copy.deepcopy(model.get_weights())\n",
    "\n",
    "        # Local Training\n",
    "        for batch_idx, (data, target) in enumerate(train_dataset):\n",
    "\n",
    "            # Minibatch Update\n",
    "            with tf.GradientTape() as tape:\n",
    "                y_p = model(data, training=True)\n",
    "                loss_val = lossF(y_true=target, y_pred=y_p)\n",
    "            grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "            optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "            acc_metric.update_state(target, y_p)\n",
    "            loss_metric.update_state(target, y_p)\n",
    "\n",
    "            # save model after local update\n",
    "            local_model = copy.deepcopy(model.get_weights())\n",
    "\n",
    "            # reset model weights\n",
    "            model.set_weights(start_model)\n",
    "\n",
    "            # Consensus Training\n",
    "            for c_batch_idx, (c_data, c_target) in enumerate(coordination_dataset):\n",
    "                with tf.GradientTape() as tape:\n",
    "                    c_yp = model(c_data, training=True)\n",
    "                    loss_val = consensus_loss(y_true=c_target, y_pred=c_yp,\n",
    "                                               z=recv_avg_pred[:, c_batch_idx].reshape(coord_batch_size, num_outputs),\n",
    "                                               l2=l2)\n",
    "                grads = tape.gradient(loss_val, model.trainable_weights)\n",
    "                optimizer.apply_gradients(zip(grads, model.trainable_variables))\n",
    "\n",
    "            # update model weights\n",
    "            average_models(model, local_model, layer_shapes, layer_sizes)\n",
    "        \n",
    "        print('(Rank %d) Epoch %d: Training Accuracy is %0.4f, Loss is %0.4f' % (rank, epoch, acc_metric.result(), loss_metric.result()))\n",
    "        loss_metric.reset_states()\n",
    "        acc_metric.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a04d2d07c24848c39dcb2b3b73eda6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:3] 2022-11-12 18:55:47.101725: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:0] 2022-11-12 18:55:47.572574: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:2] 2022-11-12 18:55:47.585755: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stderr:1] 2022-11-12 18:55:47.774726: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
       "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "train_pct = 0.8\n",
    "train_bs = 128\n",
    "test_bs = 128\n",
    "coord_bs = 32**2\n",
    "coordination_size = 32**2\n",
    "coord_num_batches = int(np.ceil(coordination_size/coord_bs))\n",
    "\n",
    "train_set, test_set, coord_set, full_train_x, full_train_y, num_inputs, num_outputs = data_pre_process(rank, size, train_pct, \n",
    "                                                                                                       train_bs, test_bs,\n",
    "                                                                                                       coordination_size, \n",
    "                                                                                                       coord_bs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "\n",
    "# initialize graph\n",
    "graph_type = 'ring'\n",
    "G = Graph(rank, size, mpi, graph_type, weight_type='uniform-symmetric', num_c=None)\n",
    "\n",
    "# initialize communicator\n",
    "communicator = DecentralizedNoModelSGD(rank, size, mpi, G)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "# multi classification model\n",
    "if rank == 0 or rank == 1:\n",
    "    multi_model = tf.keras.Sequential()\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu', input_shape=(num_inputs,)))\n",
    "    multi_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(num_outputs, activation='softmax'))\n",
    "else:\n",
    "    multi_model = tf.keras.Sequential()\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu', input_shape=(num_inputs,)))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(256, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(64, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(10, activation='relu'))\n",
    "    multi_model.add(tf.keras.layers.Dense(num_outputs, activation='softmax'))\n",
    "\n",
    "# Initialize Local Loss Function\n",
    "lossF = tf.keras.losses.SparseCategoricalCrossentropy()\n",
    "\n",
    "# model architecture\n",
    "layer_shapes, layer_sizes = get_model_architecture(multi_model)\n",
    "\n",
    "# l2 penalty\n",
    "l2 = 0.1\n",
    "\n",
    "# epochs\n",
    "epochs = 10\n",
    "\n",
    "# Initialize Optimizer\n",
    "learning_rate = 0.01\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9643a2d5353d4b83ae985cd3d4e74548",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "%px:   0%|          | 0/4 [00:00<?, ?tasks/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:1] (Rank 1) Epoch 0: Training Accuracy is 0.7287, Loss is 0.6937\n",
       "(Rank 1) Epoch 1: Training Accuracy is 0.8534, Loss is 0.4264\n",
       "(Rank 1) Epoch 2: Training Accuracy is 0.8697, Loss is 0.3554\n",
       "(Rank 1) Epoch 3: Training Accuracy is 0.8778, Loss is 0.3151\n",
       "(Rank 1) Epoch 4: Training Accuracy is 0.8814, Loss is 0.3087\n",
       "(Rank 1) Epoch 5: Training Accuracy is 0.8922, Loss is 0.2657\n",
       "(Rank 1) Epoch 6: Training Accuracy is 0.8817, Loss is 0.2958\n",
       "(Rank 1) Epoch 7: Training Accuracy is 0.8994, Loss is 0.2515\n",
       "(Rank 1) Epoch 8: Training Accuracy is 0.8992, Loss is 0.2619\n",
       "(Rank 1) Epoch 9: Training Accuracy is 0.9079, Loss is 0.2346\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:0] (Rank 0) Epoch 0: Training Accuracy is 0.7823, Loss is 1.0092\n",
       "(Rank 0) Epoch 1: Training Accuracy is 0.8442, Loss is 0.4970\n",
       "(Rank 0) Epoch 2: Training Accuracy is 0.8663, Loss is 0.3685\n",
       "(Rank 0) Epoch 3: Training Accuracy is 0.8089, Loss is 0.5461\n",
       "(Rank 0) Epoch 4: Training Accuracy is 0.8711, Loss is 0.3879\n",
       "(Rank 0) Epoch 5: Training Accuracy is 0.8752, Loss is 0.3104\n",
       "(Rank 0) Epoch 6: Training Accuracy is 0.8886, Loss is 0.2886\n",
       "(Rank 0) Epoch 7: Training Accuracy is 0.8935, Loss is 0.2710\n",
       "(Rank 0) Epoch 8: Training Accuracy is 0.8982, Loss is 0.2579\n",
       "(Rank 0) Epoch 9: Training Accuracy is 0.9039, Loss is 0.2460\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:2] (Rank 2) Epoch 0: Training Accuracy is 0.7376, Loss is 0.6879\n",
       "(Rank 2) Epoch 1: Training Accuracy is 0.8128, Loss is 0.4578\n",
       "(Rank 2) Epoch 2: Training Accuracy is 0.8632, Loss is 0.3442\n",
       "(Rank 2) Epoch 3: Training Accuracy is 0.8718, Loss is 0.3203\n",
       "(Rank 2) Epoch 4: Training Accuracy is 0.8916, Loss is 0.2687\n",
       "(Rank 2) Epoch 5: Training Accuracy is 0.8797, Loss is 0.2950\n",
       "(Rank 2) Epoch 6: Training Accuracy is 0.9037, Loss is 0.2497\n",
       "(Rank 2) Epoch 7: Training Accuracy is 0.9000, Loss is 0.2532\n",
       "(Rank 2) Epoch 8: Training Accuracy is 0.9136, Loss is 0.2234\n",
       "(Rank 2) Epoch 9: Training Accuracy is 0.9141, Loss is 0.2217\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "[stdout:3] (Rank 3) Epoch 0: Training Accuracy is 0.7401, Loss is 0.7400\n",
       "(Rank 3) Epoch 1: Training Accuracy is 0.8467, Loss is 0.4562\n",
       "(Rank 3) Epoch 2: Training Accuracy is 0.8716, Loss is 0.3425\n",
       "(Rank 3) Epoch 3: Training Accuracy is 0.8739, Loss is 0.3093\n",
       "(Rank 3) Epoch 4: Training Accuracy is 0.8794, Loss is 0.2895\n",
       "(Rank 3) Epoch 5: Training Accuracy is 0.8829, Loss is 0.2897\n",
       "(Rank 3) Epoch 6: Training Accuracy is 0.8865, Loss is 0.2732\n",
       "(Rank 3) Epoch 7: Training Accuracy is 0.8976, Loss is 0.2572\n",
       "(Rank 3) Epoch 8: Training Accuracy is 0.9051, Loss is 0.2448\n",
       "(Rank 3) Epoch 9: Training Accuracy is 0.9103, Loss is 0.2331\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%px --block\n",
    "train(multi_model, communicator, rank, size, lossF, optimizer, train_set, \n",
    "      coord_set, epochs, coord_bs, coord_num_batches, num_outputs, layer_shapes, layer_sizes, l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "predictions = multi_model.predict(full_train_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "pred = tf.math.argmax(predictions, axis=1)\n",
    "train_confusion_mtx = tf.math.confusion_matrix(full_train_y, pred)\n",
    "# normalize confusion matrix\n",
    "train_confusion_mtx = train_confusion_mtx / tf.reduce_sum(train_confusion_mtx, 0).numpy()\n",
    "train_confusion_mtx = tf.where(tf.math.is_nan(train_confusion_mtx), tf.zeros_like(train_confusion_mtx), train_confusion_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "%%px --block\n",
    "attack_labels = ['Non-Tor', 'NonVPN', 'Tor', 'VPN']\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(train_confusion_mtx,\n",
    "            xticklabels=attack_labels,\n",
    "            yticklabels=attack_labels,\n",
    "            annot=True, fmt='g')\n",
    "plt.xlabel('Prediction')\n",
    "plt.ylabel('True Label')\n",
    "plt.title('Confusion Matrix for Worker %d on CIC-Darknet2020 Data' % (rank+1))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}